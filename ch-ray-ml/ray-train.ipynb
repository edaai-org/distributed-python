{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-ray-train)=\n",
    "# Ray Train\n",
    "\n",
    "Ray Train 基于 Ray 的 Actor 和 Task，对机器学习和深度学习训练中的流程进行了封装，使得一些单机任务可以横向扩展。可以简单理解为：Ray Train 将原有的单机的机器学习工作封装在了 Actor 中，每个 Actor 有一份机器学习模型的拷贝，可独立完成机器学习训练任务；Ray Train 借助 Actor 的横向扩展能力，使得训练任务在 Ray 集群上横向扩展。Ray Train 对 PyTorch、PyTorch Lightning、Hugging Face Transformers、XGBoost、LightGBM 等常用机器学习库进行了封装，并向用户提供了一些接口，用户不需要自己编写 Actor 代码，只需要稍微改动原有的单机机器学习工作流，即可快速切换到集群模式。本节以 PyTorch 为例，介绍如何基于数据并行将训练任务横向扩展，关于数据并行的原理，可参考 {numref}`sec-data-parallel`。\n",
    "\n",
    "## 关键步骤\n",
    "\n",
    "将一个 PyTorch 单机训练代码修改为 Ray Train 需要做以下修改：\n",
    "\n",
    "* 定义 `train_loop`，它是一个单节点训练的函数，包括加载数据，更新参数。\n",
    "* 定义 [`ScalingConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html)，它定义了如何横向扩展这个训练作业，包括需要多少个计算节点，是否使用 GPU 等。\n",
    "* 定义 `Trainer`，把 `train_loop` 和 `ScalingConfig` 粘合起来，然后执行 `Trainer.fit()` 方法进行训练。\n",
    "\n",
    "{numref}`fig-ray-train-key-parts` 展示了适配 Ray Train 的关键部分。\n",
    "\n",
    "```{figure} ../img/ch-ray-ml/ray-train-key-parts.svg\n",
    "---\n",
    "width: 500px\n",
    "name: fig-ray-train-key-parts\n",
    "---\n",
    "Ray Train 关键部分\n",
    "```\n",
    "\n",
    "具体的代码主要包括：\n",
    "\n",
    "```python\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "def train_loop():\n",
    "    ...\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=..., use_gpu=...)\n",
    "trainer = TorchTrainer(train_loop_per_worker=train_loop, scaling_config=scaling_config)\n",
    "result = trainer.fit()\n",
    "```\n",
    "\n",
    "## 案例：图像分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个完整的训练，这个例子使用了 PyTorch 提供的 ResNet 模型 {cite}`he2016DeepResidualLearning`，读者可以根据自己环境中的 GPU 数量，设置 `ScalingConfig`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import ray\n",
    "import ray.train.torch\n",
    "from ray.train import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(model, optimizer, criterion, train_loader):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # 无需手动将 images 和 labels 发送到指定的 GPU 上\n",
    "        # `prepare_data_loader` 帮忙完成了这个过程\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test_func(model, data_loader):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            # data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"../data\")\n",
    "\n",
    "def train_loop():\n",
    "    # 加载数据并进行数据增强\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor(), \n",
    "         torchvision.transforms.Normalize((0.5,), (0.5,))]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        torchvision.datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform),\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        torchvision.datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform),\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n",
    "\n",
    "    # 1. 将数据分发到多个计算节点\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "    test_loader = ray.train.torch.prepare_data_loader(test_loader)\n",
    "    \n",
    "    # 原始的 resnet 为 3 通道的图像设计的\n",
    "    # FashionMNIST 为 1 通道，修改 resnet 第一层以适配这种输入\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    \n",
    "    # 2. 将模型分发到多个计算节点和 GPU 上\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 训练 10 个 epoch\n",
    "    for epoch in range(10):\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        train_func(model, optimizer, criterion, train_loader)\n",
    "        acc = test_func(model, test_loader)\n",
    "        \n",
    "        # 3. 监控训练指标和保存 checkpoint\n",
    "        metrics = {\"acc\": acc, \"epoch\": epoch}\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics,\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-10 09:41:32</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:33.99        </td></tr>\n",
       "<tr><td>Memory:      </td><td>31.5/90.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/64 CPUs, 4.0/4 GPUs (0.0/1.0 accelerator_type:TITAN)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   acc</th><th style=\"text-align: right;\">  epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_3d3d1_00000</td><td>TERMINATED</td><td>10.0.0.3:49324</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         80.9687</td><td style=\"text-align: right;\">0.8976</td><td style=\"text-align: right;\">      9</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(RayTrainWorker pid=49400)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m - (ip=10.0.0.3, pid=49399) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m - (ip=10.0.0.3, pid=49400) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m - (ip=10.0.0.3, pid=49401) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=49324)\u001b[0m - (ip=10.0.0.3, pid=49402) world_rank=3, local_rank=3, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Wrapping provided model in DistributedDataParallel.\n",
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m [rank2]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=49400)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=49402)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=49402)\u001b[0m [rank3]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8604, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8808, 'epoch': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000002)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8852, 'epoch': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000003)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8964, 'epoch': 3}\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8972, 'epoch': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000004)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000005)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8968, 'epoch': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000006)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8948, 'epoch': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000007)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.894, 'epoch': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000008)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.894, 'epoch': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49401)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name/TorchTrainer_3d3d1_00000_0_2024-04-10_09-39-58/checkpoint_000009)\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=49399)\u001b[0m {'acc': 0.8976, 'epoch': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 09:41:32,109\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-04-10 09:41:32,112\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/home/u20200002/distributed-python/ch-ray-train-tune/../data/torch_ckpt/experiment_name' in 0.0057s.\n",
      "2024-04-10 09:41:32,120\tINFO tune.py:1048 -- Total run time: 94.05 seconds (93.99 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# 4. 配置 `ScalingConfig`，Ray Train 根据这个配置将训练任务拓展到集群\n",
    "scaling_config = ray.train.ScalingConfig(num_workers=4, use_gpu=True)\n",
    "\n",
    "# 5. 使用 TorchTrainer 启动并行训练\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_loop_per_worker=train_loop,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=ray.train.RunConfig(\n",
    "        storage_path=os.path.join(data_dir, \"torch_ckpt\"),\n",
    "        name=\"exp_fashionmnist_resnet18\",\n",
    "    )\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与原生 PyTorch 的区别\n",
    "\n",
    "### 与单机程序的区别\n",
    "\n",
    "Ray Train 帮用户将模型和数据分发到多个计算节点，用户需要设置：`model = ray.train.torch.prepare_model(model)` 和 `train_loader = ray.train.torch.prepare_data_loader(train_loader)`，设置之后，Ray Train 不需要显式地调用 `model.to(\"cuda\")`，也不需要 `images, labels = images.to(\"cuda\"), labels.to(\"cuda\")` 等将模型数据拷贝到 GPU 的代码。\n",
    "\n",
    "### 与 `DistributedDataParallel` 的区别\n",
    "\n",
    "PyTorch 的 `DistributedDataParallel` 也可以实现数据并行，Ray Train 把 `DistributedDataParallel` 中的复杂细节都隐藏起来，只需要用户从单机代码稍作改动，不需要 `torch.distributed` 的分布式环境（World）和进程（Rank）。有关 World 和 Rank 等概念，可以参考 {numref}`sec-mpi-hello-world`。\n",
    "\n",
    "## 数据读取\n",
    "\n",
    "如果单机版的数据读取是基于 PyTorch 的 `DataLoader`，可以使用 [`ray.train.torch.prepare_data_loader()`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html) 对原有的 PyTorch `DataLoader` 进行适配。也可以使用 Ray Data 提供的数据预处理方法进行数据预处理。\n",
    "\n",
    "## `ScalingConfig`\n",
    "\n",
    "`ScalingConfig(num_workers=..., use_gpu=...)` 的 `num_workers` 设置让当前的任务以多大程度并行，`use_gpu` 用来分配 GPU。`num_workers` 可以理解成启动多少个 Ray Actor，每个 Actor 独立地执行单机训练任务。如果 `use_gpu=True`，默认给每个 Actor 分配 1 个 GPU，每个 Actor 看到的 `CUDA_VISIBLE_DEVICES` 也是 1 个。如果希望每个 Actor 看到多个 GPU，可以设置：`resources_per_worker={\"GPU\": n}`。\n",
    "\n",
    "## 监控\n",
    "\n",
    "分布式训练中，每个 Worker 是独立运行的，但大部分情况下，只需要对进程号（Rank）为 0 的第一个进程监控即可。`ray.train.report(metrics=...)` 默认收集 Rank=0 的指标。\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "Checkpoint 的过程大致如下：\n",
    "\n",
    "1. Checkpoint 会先写到本地的目录，可以直接用 PyTorch、PyTorch Lightning 或 TensorFlow 提供的保存模型的接口。比如刚才例子中的：\n",
    "\n",
    "```\n",
    "with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "    torch.save(\n",
    "        model.module.state_dict(),\n",
    "        os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "    )\n",
    "```\n",
    "\n",
    "1. 在 `ray.train.report(metrics=..., checkpoint=...)` 时，将刚刚保存在本地的 Checkpoint 上传到持久化文件系统（比如，S3 或者 HDFS）中，该文件系统可被所有计算节点访问。本地的 Checkpoint 只是一个缓存，Checkpoint 上传到持久化文件系统后，本地的 Checkpoint 会被删除。持久化的文件系统目录在 `TorchTrainer` 上配置：\n",
    "\n",
    "```{code-block} python\n",
    ":name: ray-train-persistent-storage\n",
    ":emphasize-lines: 5\n",
    "\n",
    "TorchTrainer(\n",
    "    train_loop,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=ray.train.RunConfig(\n",
    "        storage_path=...,\n",
    "        name=\"experiment_name\",\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "使用数据并行训练时，每个 Rank 都有一份模型权重的拷贝，保存到本地与持久化文件系统上的 Checkpoint 是一样的。使用流水线并行训练（{numref}`sec-pipeline-parallel`）等其他并行策略时，每个 Rank 的本地保存的是模型的一部分，每个 Rank 分别保存自己那部分的模型权重。生成 Checkpoint 文件时，应该加上一些文件前后缀，以做区分。\n",
    "\n",
    "```{code-block} python\n",
    ":name: ray-train-distributed-checkpoint\n",
    ":emphasize-lines: 2,5\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "    rank = train.get_context().get_world_rank()\n",
    "    torch.save(\n",
    "        ...,\n",
    "        os.path.join(temp_checkpoint_dir, f\"model-rank={rank}.pt\"),\n",
    "    )\n",
    "    train.report(\n",
    "        metrics, \n",
    "        checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
