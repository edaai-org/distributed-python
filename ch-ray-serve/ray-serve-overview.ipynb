{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-ray-serve-overview)=\n",
    "# Ray Serve\n",
    "\n",
    "Ray Serve 是基于 Ray 的模型推理服务框架。一个生产可用的模型推理系统通常比较有很长的链路：\n",
    "\n",
    "* 对外服务 API：可能是 HTTP 的，也可能基于远程过程调用协议（Remote Procedure Call，RPC）的。对外服务的 API 又被称为访问入口（Ingress）。其他用户或者程序通过 Ingress 来访问模型推理服务。HTTP 服务可以使用 [FastAPI](https://fastapi.tiangolo.com/)，RPC 可以使用 [gRPC](https://grpc.io/)。\n",
    "* 模型推理引擎：给定模型和输入，进行模型推理。模型推理引擎可能是成熟的深度学习框架，比如 PyTorch 或者 TensorFlow；可能是针对某个领域的高性能推理框架，比如针对大语言模型的 [vLLM](https://github.com/vllm-project/vllm)，或者是厂商自研的。\n",
    "* 输入输出处理模块：对输入数据进行必要的特征预处理，或对模型输出进行一定的后处理。以输入数据预处理为例，数据可能存储在数据库中，需要经过预处理才能交付给机器学习模型。\n",
    "* 多模型：大型项目通常需要多个模型共同协作。比如短视频推荐系统有多个模块：从海量视频素材中召回用户最感兴趣的少量内容，使用不同的模型预测用户的点击概率或停留时长，结合多种不同的指标对少量内容进行排序。\n",
    "\n",
    "Ray Serve 基于 Ray Task 和 Actor 的并行能力，解决模型推理中的痛点：\n",
    "\n",
    "* 集成了常见的 Ingress，比如 FastAPI 和 gRPC。\n",
    "* 底层基于 Ray Task 和 Actor，可封装任何 Python 推理引擎，并支持横向扩展和负载均衡。\n",
    "* Python 接口允许开发者进行敏捷开发，它像胶水一样，能够定义复杂的模型推理链路，将不同的数据源、推理引擎和模型粘合在一起。\n",
    "\n",
    "## 关键概念\n",
    "\n",
    "Ray Serve 中有两个关键概念：部署（Deployment）和应用程序（Application）。Application 是一个完整的推理应用；Deployment 可以理解成整个推理应用的某个子模块，比如，对于输入数据进行预处理，或使用机器学习模型进行预测。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
