{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-ray-data-preprocessor)=\n",
    "# Preprocessor\n",
    "\n",
    "{numref}`sec-ray-data-transform` 介绍了通用接口 `map()` 和 `map_batches()`。对于结构化的表格类数据，Ray Data 在 `map()` 和 `map_batches()` 基础上，增加了一个高阶的 API：预处理器（Preprocessor）。[Preprocessor](https://docs.ray.io/en/latest/data/api/preprocessor.html) 是一系列特征处理操作，可与机器学习模型训练和推理更好地结合。其使用方式与 scikit-learn 的 [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) 非常相似，熟悉 scikit-learn 的用户可以快速迁移过来。对于非结构化数据，比如图片、视频等，仍然建议使用 `map()` 或者 `map_batches()`。\n",
    "\n",
    "## 使用\n",
    "\n",
    "Preprocessor 主要有 4 类操作：\n",
    "\n",
    "1. [`fit()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessor.Preprocessor.fit.html)：计算 Ray Data `Dataset` 状态信息，比如计算某一列数据的方差或者均值。\n",
    "2. [`transform()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessor.Preprocessor.transform.html)：执行转换操作。如果这个转换操作是有状态的，那必须先进行 `fit()`。\n",
    "3. [`transform_batch()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessor.Preprocessor.transform_batch.html)：对一个批次数据进行转换操作。\n",
    "4. [`fit_transform()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessor.Preprocessor.fit_transform.html)：结合了 `fit()` 和 `transform()` 的一个操作，先对 `Dataset` 进行 `fit()`，再进行 `transform()`。\n",
    "\n",
    "下面根据出租车数据集，来演示一下如何使用 Preprocessor。出租车数据是一个典型的结构化数据，里面有很多列，比如该旅程的距离，这些列可被用来作为机器学习算法的特征，而喂给机器学习模型前，需要进行特征处理。比如 [`MinMaxScaler`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessors.MinMaxScaler.html) 将特征进行归一化：\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-15 14:07:48,341\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-12-15 14:07:50,654\tINFO worker.py:1673 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 /Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/../data/nyc-taxi 已存在，无需操作。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "from typing import Any, Dict\n",
    "\n",
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()\n",
    "\n",
    "folder_path = os.path.join(os.getcwd(), \"../data/nyc-taxi\")\n",
    "download_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet\"\n",
    "file_name = download_url.split(\"/\")[-1]\n",
    "parquet_file_path = os.path.join(folder_path, file_name)\n",
    "if not os.path.exists(folder_path):\n",
    "    # 创建文件夹\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"文件夹 {folder_path} 不存在，已创建。\")\n",
    "    # 下载并保存 Parquet 文件\n",
    "    with urllib.request.urlopen(download_url) as response, open(parquet_file_path, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "    print(\"数据已下载并保存为 Parquet 文件。\")\n",
    "else:\n",
    "    print(f\"文件夹 {folder_path} 已存在，无需操作。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=6866) Parquet Files Sample 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 14:17:24,489\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=173 for stage ReadParquet to satisfy output blocks of size at least DataContext.get_current().target_min_block_size=1.0MiB.\n",
      "2023-12-15 14:17:24,491\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 173, each read task output is split into 173 smaller blocks.\n",
      "2023-12-15 14:17:24,491\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=1]\n",
      "2023-12-15 14:17:24,492\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 14:17:24,493\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "                                                                                                                         \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'trip_distance': 3.4}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data.preprocessors import MinMaxScaler\n",
    "\n",
    "ds = ray.data.read_parquet(parquet_file_path,\n",
    "    columns=[\"trip_distance\"])\n",
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过 `MinMaxScaler` 归一化之后，原来的值变为一个归一化之后的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 14:17:29,924\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=173 for stage ReadParquet to satisfy output blocks of size at least DataContext.get_current().target_min_block_size=1.0MiB.\n",
      "2023-12-15 14:17:29,925\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 173, each read task output is split into 173 smaller blocks.\n",
      "2023-12-15 14:17:29,926\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n",
      "2023-12-15 14:17:29,927\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 14:17:29,928\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Running: 0.0/8.0 CPU, 0.0/0.0 GPU, 0.0 MiB/512.0 MiB object_store_memory:   0%|          | 0/173 [00:00<?, ?it/s]   \n",
      "\u001b[A\n",
      "\n",
      "                                                                                                                       \n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A2023-12-15 14:17:31,195\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=173 for stage ReadParquet to satisfy output blocks of size at least DataContext.get_current().target_min_block_size=1.0MiB.\n",
      "2023-12-15 14:17:31,196\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 173, each read task output is split into 173 smaller blocks.\n",
      "2023-12-15 14:17:31,197\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(MinMaxScaler._transform_pandas)] -> LimitOperator[limit=1]\n",
      "2023-12-15 14:17:31,198\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 14:17:31,200\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(ReadParquet->SplitBlocks(173) pid=6869)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(173) pid=6869)\u001b[0m   return transform_pyarrow.concat(tables)                         \n",
      "                                                                                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'trip_distance': 1.8353531664835362e-05}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = MinMaxScaler(columns=[\"trip_distance\"])\n",
    "preprocessor.fit(ds)\n",
    "minmax_ds = preprocessor.transform(ds)\n",
    "minmax_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/preprocessor.py:125: UserWarning: `fit` has already been called on the preprocessor (or at least one contained preprocessors if this is a chain). All previously fitted state will be overwritten!\n",
      "  warnings.warn(\n",
      "2023-12-15 14:51:29,990\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=173 for stage ReadParquet to satisfy output blocks of size at least DataContext.get_current().target_min_block_size=1.0MiB.\n",
      "2023-12-15 14:51:29,993\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 173, each read task output is split into 173 smaller blocks.\n",
      "2023-12-15 14:51:29,995\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=1]\n",
      "2023-12-15 14:51:29,997\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 14:51:29,998\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Running: 0.0/8.0 CPU, 0.0/0.0 GPU, 0.0 MiB/512.0 MiB object_store_memory:   0%|          | 0/173 [00:00<?, ?it/s]   \n",
      "\u001b[A\n",
      "\n",
      "                                                                                                                       \n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A2023-12-15 14:51:31,829\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=173 for stage ReadParquet to satisfy output blocks of size at least DataContext.get_current().target_min_block_size=1.0MiB.\n",
      "2023-12-15 14:51:31,833\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 173, each read task output is split into 173 smaller blocks.\n",
      "2023-12-15 14:51:31,836\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(MinMaxScaler._transform_pandas)] -> LimitOperator[limit=1]\n",
      "2023-12-15 14:51:31,838\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 14:51:31,840\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(ReadParquet->SplitBlocks(173) pid=6870)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(173) pid=6870)\u001b[0m   return transform_pyarrow.concat(tables)                         \n",
      "                                                                                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'trip_distance': 1.8353531664835362e-05}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2023-12-15 15:24:05,019 E 6858 18306341] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2023-12-15_14-07-48_510959_95090 is over 95% full, available space: 49982570496; capacity: 1000240963584. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "minmax_ds_ft = preprocessor.fit_transform(ds)\n",
    "minmax_ds_ft.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类变量和数值变量\n",
    "\n",
    "### 分类变量\n",
    "\n",
    "机器学习模型无法接受分类变量，所以需要进行一些转换。{numref}`tab-categorical-data-preprocessor` 是几个处理分类变量的 Preprocessor。\n",
    "\n",
    "```{table} 用于处理分类变量的 Preprocessor\n",
    ":name: tab-categorical-data-preprocessor\n",
    "\n",
    "|    Preprocessor   \t| 变量类型 \t |                案例                \t|\n",
    "|:-----------------:\t|:--------:\t|:----------------------------------: |\n",
    "|   [`LabelEncoder`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessors.LabelEncoder.html)  \t| 无序分类 \t |           猫，狗，牛，羊           \t  |\n",
    "|   [`OrdinalEncoder`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessors.OrdinalEncoder.html)  \t| 有序分类 \t | 高中，本科，硕士，博士             \t   |\n",
    "|   [`MultiHotEncoder`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessors.MultiHotEncoder.html) \t| 多分类   \t | [\"喜剧\", \"动画\"], [\"悬疑\", \"动作\"]     |\n",
    "```\n",
    "\n",
    "### 数值变量\n",
    "\n",
    "使用下面的转换将数据进行转换，以适应特定的机器学习模型，{numref}`tab-numerical-data-preprocessor` 是几个处理数值变量的 Preprocessor。\n",
    "\n",
    "```{table} 用于处理数值变量的 Preprocessor\n",
    ":name: tab-numerical-data-preprocessor\n",
    "\n",
    "| Preprocessor       \t| 变量类型             \t| 计算方式                                   \t| 备注                                                     \t|\n",
    "|--------------------\t|----------------------\t|--------------------------------------------\t|----------------------------------------------------------\t|\n",
    "| [`RobustScaler`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessors.RobustScaler.html)     \t| 有离群值             \t| $x' = \\frac{x - \\mu_{1/2}}{\\mu_h - \\mu_l}$ \t| $\\mu_{1/2}$ 是中位数，$\\mu_h$ 是最大值，$\\mu_l$ 是最小值 \t|\n",
    "| [`MaxAbsScaler`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessors.MaxAbsScaler.html)     \t| 数据稀疏             \t| $x' = \\frac{x}{\\max{\\vert x \\vert}}$       \t|                                                          \t|\n",
    "| [`PowerTransformer`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessors.PowerTransformer.html) \t| 将数据变为正太分布   \t| Yeo-Johnson 或 Box-Cox                     \t|                                                          \t|\n",
    "| [`Normalizer`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessors.Normalizer.html)       \t| 需要对数据进行正则化 \t| $x' = \\frac{x}{\\lVert x \\rVert_p}$         \t| $p$ 是正则方式，比如 `l1` 正则是绝对值求和               \t|\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
