{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-xinference)=\n",
    "# Xinference\n",
    "\n",
    "Xorbits Inference (Xinference) 是一款面向大模型的推理平台，支持大语言模型、向量模型、文生图模型等。它底层基于 [Xoscar](https://github.com/xorbitsai/xoscar) 提供的分布式能力，使得模型可以在集群上部署，上层提供了类 OpenAI 的接口，用户可以在上面部署和调用开源大模型。Xinference 将对外服务的 API、推理引擎和硬件做了集成，比 Ray Serve 更简单。\n",
    "\n",
    "## 推理引擎\n",
    "\n",
    "Xinference 可适配不同推理引擎，包括 Hugging Face Transformers、vLLM、GGML（llama.cpp）等，因此在安装时也要安装对应的推理引擎，比如 `pip install \"xinference[transformers]\"`。Transformers 完全基于 PyTorch，适配的模型最快最全，但性能较差；其他推理引擎，比如 vLLM、GGML 专注与性能优化，但模型覆盖度没 Transformers 高。\n",
    "\n",
    "## 集群\n",
    "\n",
    "使用之前需要先启动一个 Xinference 推理集群，可以是单机多卡，也可以是多机多卡。单机上可以在命令行里这样启动：\n",
    "\n",
    "```shell\n",
    "xinference-local --host 0.0.0.0 --port 9997\n",
    "```\n",
    "\n",
    "集群场景与 Xorbits Data 类似，先启动一个 Supervisor，再启动 Worker：\n",
    "\n",
    "```shell\n",
    "# 启动 Supervisor\n",
    "xinference-supervisor -H <supervisor_ip>\n",
    "\n",
    "# 启动 Worker\n",
    "xinference-worker -e \"http://<supervisor_ip>:9997\" -H <worker_ip>\n",
    "```\n",
    "\n",
    "之后就可以在 http://<supervisor_ip>:9997 访问 Xinference 服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
